# Blog [Adventures in Async](https://medium.com/darklang/adventures-in-async-25cde0d40dd8)

Async:
* Allowing servers to respond to many more requests by taking advantage of non-blocking IO
* Non-blocking IO, a single thread can handle many many requests

* OCaml LWT

* For languages that don't have garbage collection, managing your own memory increases complexity in an asynchronous server.

# Blog [Asynchronous Python and Database](https://techspot.zzzeek.org/2015/02/15/asynchronous-python-and-databases/)

* Python two ways to avoid callback spaghetti
  * implicit async IO approach (gevent)
    1. a system of green threads which are scheduled by a runtime library or virtual machine instead of native IO thread
    2. using a library like libev to schedule work between green threads when non-blocking IO is invoked
  * asyncio library

* classic use case of async programming
  * for a server tending to lots of usually asleep or arbitrarily slow connections

* Javascript
  * at the beginning, no concept at all of multi-threaded programming 
  * strong concept of an event loop with callbacks

* Python
  * asyncio to avoid callback spaghetti

Issue One - Async as Magic Performance Fairy Dust

# Blog [Async IO on Linux](https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/)

* `epoll` group
  * epoll_create
  * epoll_ctl
  * epoll_wait
  * give the Linux kernel a list of file descriptors to track and ask for updates
  * epoll could be used when you're using green threads or event loop to do all your networking & pipe I/O

# Research about the async
  - [x] ocaml discussion
  - [x] lwt manual
  - [x] python talk on async/what's an event loop
  - [x] async TS compile to state machine JS
  - [x] c++ coroutine

## [Ocaml Discussion](https://discuss.ocaml.org/t/lwt-vs-system-threads/5007)

* one thread per connection
* benchmark suggestion
  * Fix a maximal request rate you want to support
  * Fix dummy per request workload
* OCaml's global runtime lock could nullify the advantages of multi-threading
  * leaving you only with the cost of context-switches vs epoll+management code
* go coroutine implementation
  * Goroutines are implemented more or less as coroutines, but the scheduler multiplexes them onto a thread pool. 
* multi-core ocaml PR enable true parallelism in addition to concurrency
  * https://github.com/ocaml-multicore/ocaml-multicore/pull/381
* [Linux Application Performance Introduction](https://unixism.net/2019/04/linux-applications-performance-introduction/)
  * `poll()` or `epoll_wait()` will tell you when a request is ready move from one **state** to another
  * With event-driven servers, it is as if you're suspending one client request in a particular state when it blocks and switch to another client request which could be in a different state
  * bottleneck of poll is that it need to a linear search of tracked file descriptors
    * loop through the entire array of file descriptors even if there is only one descriptor that is actually ready
  * main difference between epoll and poll based server is the API

## [LWT Manual](https://ocsigen.org/lwt/latest/manual/manual)
* Promise based solution
* preemptive system threads 
  * doing it right with threads is a difficult job
  * system threads consume non-negligible resources
* promise creation
  * Lwt.wait : unit -> 'a Lwt.t * 'a Lwt.u where Lwt.u is the resolver of the promise
* promise composition 
  * bind
* cancelable promises
* concurrent composition
  * join/choose
* Lwt.unix
  * non-blocking system call
* Lwt scheduler
  * Operation doing I/O have to be resumed when some events are received by the process, so they can resolve their associated pending promises.
  * [I/O Multiplexing 1](https://notes.shichao.io/unp/ch6/)
  * I/O multiplexing means what it says - allowing the programmer to examine and block on multiple I/O streams
    * poll based model may waste cpu cycles
  * libev
  * select
* `Lwt_main.run : 'a Lwt.t -> 'a`
  * The Lwt_main module contains the main loop of Lwt
  * This function continuously runs the scheduler until the promise passed as argument is resolved.
* Lwt has a thread pool
  * to detach computation to preemptive threads
  * For operation that cannot be executed asynchronously

## [Python What's async](https://www.youtube.com/watch?v=kdzL3r-yJZY)

* throughput are bounded by computation
* Minimize resources per idle connection
* blocking IO
  * IO results are stored in the current stack frame
  * what shall be done next is remembered by program counter
* non-blocking IO 
  * callback
  * no program counter
* ways to store state:
  * spectrum
    * thread - greenlet - coroutines - callbacks
* what's async?
  * single-threaded?
  * I/O concurrency
  * Non-blocking sockets to achieve
  * epoll/kqueue (event notification when results are ready)
  * event loop
    * to use epoll/kqueue to ask whether some network event has happened
    * if so, execute the callback
    * repeat the loop forever
* When shall not to use async?
  * doesn't have async driver
    * you shall not use a blocking IO in async

* what's the difference between coroutine and async? 
  * coroutine - light weight threads

### JS event loop

* V8
  * call stack
  * heap

* one thread == one call stack == one thing at a time
* stack trace: the state of stack when the error happens
* event loop
  * setTimeOut 0
  * defer the execution of a task until the stack is clear

### Python coroutine

* coroutine allow multiple entry points for suspending and resuming execution at certain location 

## async TS compile to state machine JS

stackful v.s. stackless

demonstration of stack
```js
async function foo()
{
    await 1;
    console.log("hi")
    await 2;
    await 3;
    console.log("bye")
}
```

do the syntactic transformation to a state machined based code

## [Scalable I/O: Events vs Multi-threading based](https://thetechsolo.wordpress.com/2016/02/29/scalable-io-events-vs-multithreading-based/)
* some fact
  * user space to kernel space => light weight context switch

* Scalable I/O 
  * Single threaded non-blocking I/O
    * bottleneck in the event loop
  * Multi threaded non-blocking I/O
    * bottleneck in the event loop
    * better performance for CPU-bound tasks
    * paradigm
      *  a single dispatcher Thread/Process pulls the Events and dispatch them over the Worker Threads/Processes.
  * blocking multi threaded

* why should an application re-code what the Kernel is supposed to do?

## C++ coroutine

Normal function
  * Call - create an activation frame. suspend the caller
  * Return - destroy an activation frame. return value to the caller and resume caller's operations

Activation Frames
  * the block of memory holds the current state of a particular invocation of a function
  * the values of any parameters
  * the values of any local variables

For Normal function, all activation frames have strictly nested lifetimes => highly efficient memory allocation data-structure == stack

Coroutine
  * Call
  * Return
  * Suspend
  * Resume
  * Destroy

Activation frame lifetimes are not strictly nested

* suspend operation
  * are identified by usages of the `co_await` or `co_yield` keywords
* resume
  * call resume method on the coroutine-frame handle

* Understanding operator `co_await`
  * Promise interface
  * Awaitable interface

An Awaiter type is a type that implements three special methods that are called as part of a `co_await` expression:
  * await_ready
  * await_suspend
  * await_resume

Compilers
  * take a `co_await` expression to generate
  * First thing: generate code to obtain the Awaiter object for the awaited value.

What makes a function a coroutine?
  * if it contains `co_await`, `co_yield` or `co_return`


* `co_await` is used to suspend the coroutine and return control to its caller
* How does a caller resume a coroutine?
  * coroutine frame occurs before the coroutine starts running
  * The compiler "returns" a handle to this coroutine frame to the caller of the coroutine
  * coroutine_handle<void>
  * template <typename Promise> struct coroutine_handle : coroutine_handle<void>
    * Promise& promise() const noexcept
    * static coroutine_handle from_promise(Promise&) except

* Let's look at future
* future<int> compute_value()
  * Make future a coroutine type (promise)
  * Make future awaitable

## Cooperative thread

[SO](https://softwareengineering.stackexchange.com/questions/254140/is-there-a-difference-between-fibers-coroutines-and-green-threads-and-if-that-i)
* A fiber is a lightweight thread that uses cooperative multitasking
* A coroutine generalizes a subroutine to allow multiple entry points for suspending and resuming execution 
* A green thread is a thread that is scheduled by runtime instead of natively by the underlying operating system

[hackernew](https://news.ycombinator.com/item?id=14439615)
* 1:N green threads (provide concurrency only)
* M:N green threads (Erlang  model - provide parallelism and concurrency)
* Erlang model have green thread spawned for every requests.
* message-passing actor model allow erlang to write synchronous code but outwardly it's all async
* Fibers are a way of switching program counter and context explicitly in userland
* Haskell has green (N to M) threads that are pre-emptive

[Green Threads Explained](https://c9x.me/articles/gthreads/intro.html)
* Magic of green threads: we're able to express multiple independent processes that run "at the same time"

[Distinguishing coroutines and fibers](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4024.pdf)
* In effect, fibers extend the concurrency taxonomy:
  * on a single computer, multiple processes can run
  * within a single process, multiple threads can run
  * within a single thread, multiple fibres can run
* Coroutine
  * In all the above examples, as in every coroutine usage, the handshake between producer and consumer is direct and immediate.
  * The coroutine library provides no facilities for synchronizing coroutines: coroutines are already synchronous.

[LWT: a Cooperative Thread Library](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.224.745&rep=rep1&type=pdf)
* Two-folds
  * avoid race condition due to explicit context switch 
    * yield or blocking system calls
  * light-weight threads hence good for concurrency

[Green Threads are like Garbage Collection](https://www.fpcomplete.com/blog/2017/01/green-threads-are-like-garbage-collection/)
* Languages like Haskell, Erlang, and Go provide a similar separation-of-concern in the form of green threads. Instead of requiring the developer to manually deal with asynchronous (or non-blocking) I/O calls, the runtime system takes responsibility for this
* [disadvantages of green-threads](https://www.reddit.com/r/haskell/comments/5mm93r/what_are_the_inherent_disadvantages_of_green/)